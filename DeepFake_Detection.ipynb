{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFake Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nesea\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.5\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy ultralytics matplotlib opencv-python tensorflow scikit-learn seaborn keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import RandomRotation, RandomFlip, RandomZoom, GaussianNoise , RandomContrast, RandomBrightness, Resizing, Rescaling, Conv2D, BatchNormalization, MaxPool2D, SeparableConv2D, ReLU, Add, GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Recall, AUC\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.train import BytesList, Int64List\n",
    "from tensorflow.train import Example, Features, Feature\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATION = {\n",
    "    \"BATCH_SIZE\" : 32,\n",
    "    \"IMAGE_SIZE\" : 299,\n",
    "    \"BETA_1\": 0.9,\n",
    "    \"BETA_2\": 0.999,\n",
    "    \"EPSILON\": 1e-08,\n",
    "    \"LEARNING_RATE\": 0.0002,\n",
    "    \"EPOCHS\": 10,\n",
    "    \"SEED\": 42,\n",
    "    \"CLASS_NAMES\": [\"original\", \"manipulated\"],\n",
    "    \"STEPS_PER_EPOCH\": 29197,\n",
    "    \"VALIDATION_STEPS\": 3649\n",
    "}\n",
    "\n",
    "original_sequences_directory = \"videos/original_sequences/youtube/c23/videos\"\n",
    "deepFakes_directory = \"videos/manipulated_sequences/Deepfakes/c23/videos\"\n",
    "face2Face_directory = \"videos/manipulated_sequences/Face2Face/c23/videos\"\n",
    "faceSwap_directory = \"videos/manipulated_sequences/FaceSwap/c23/videos\"\n",
    "neuralTexture_directory = \"videos/manipulated_sequences/NeuralTextures/c23/videos\"\n",
    "output_original_frame_directory = \"frames/original\"\n",
    "output_face2face_directory = \"frames/manipulated/face2face\"\n",
    "output_deepfakes_directory = \"frames/manipulated/deepfakes\"\n",
    "output_faceswap_directory = \"frames/manipulated/faceswap\"\n",
    "output_neuraltexture_directory = \"frames/manipulated/neuraltexture\"\n",
    "\n",
    "logging.getLogger('ultralytics').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download del dataset FaceForensics++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKtDot2W-sky",
    "outputId": "76f05d7b-03ff-439b-f868-90ef95a98cf1"
   },
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/original --server EU2 -t videos -c c23 -d original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/original --server EU2 -t videos -c c40 -d original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoPuhPYxFCC-",
    "outputId": "b46d98e0-3304-42df-bdb9-345a2b7251cb"
   },
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/Face2Face --server EU2 -t videos -c c23 -d Face2Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/Face2Face --server EU2 -t videos -c c40 -d Face2Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/DeepFake  --server EU2 -t videos -c c23 -d Deepfakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QtJNJOjH1sv",
    "outputId": "0db7e593-8177-44b9-f270-900545dae476"
   },
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/DeepFake  --server EU2 -t videos -c c40 -d Deepfakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/FaceSwap --server EU2 -t videos -c c23 -d FaceSwap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypXYwk1XJds_",
    "outputId": "a891713d-a358-4c5d-e1aa-bcbbef02c67e"
   },
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/FaceSwap --server EU2 -t videos -c c40 -d FaceSwap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/NeuralTexture  --server EU2 -t videos -c c23 -d NeuralTextures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a09zuhxVn_-W",
    "outputId": "0ef92cac-3495-4bea-c3a7-67df1e47c91e"
   },
   "outputs": [],
   "source": [
    "%run \"download-FaceForensics.py\" videos/NeuralTexture  --server EU2 -t videos -c c40 -d NeuralTextures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "base_path = \"downloaded_videos\"\n",
    "\n",
    "original_path = os.path.join(base_path, \"original_sequences\")\n",
    "manipulated_path = os.path.join(base_path, \"manipulated_sequences\")\n",
    "\n",
    "dest_dir = \"datasets\"\n",
    "subdirs = [\"training_set\", \"validation_set\", \"test_set\"]\n",
    "categories = [\"original\", \"manipulated\"]\n",
    "\n",
    "for subdir in subdirs:\n",
    "    for category in categories:\n",
    "        os.makedirs(os.path.join(dest_dir, subdir, category), exist_ok=True)\n",
    "\n",
    "def copy_videos(src_path, dest_base, count):\n",
    "    \"\"\"Copia i video dalla sorgente alla destinazione basandosi su intervalli di conteggio.\"\"\"\n",
    "    videos = sorted([f for f in os.listdir(src_path) if f.endswith(\".mp4\")])  # Filtra i file video\n",
    "    splits = {\"training_set\": videos[:800], \n",
    "              \"validation_set\": videos[800:900], \n",
    "              \"test_set\": videos[900:]}\n",
    "    \n",
    "    for split, video_list in splits.items():\n",
    "        dest_path = os.path.join(dest_base, split)\n",
    "        for video in video_list:\n",
    "            shutil.copy(os.path.join(src_path, video), dest_path)\n",
    "\n",
    "original_videos_path = os.path.join(original_path, \"c23/videos\")\n",
    "for subdir in subdirs:\n",
    "    copy_videos(original_videos_path, os.path.join(dest_dir, subdir, \"original\"), 1000)\n",
    "\n",
    "manipulated_dirs = [d for d in os.listdir(manipulated_path) if os.path.isdir(os.path.join(manipulated_path, d))]\n",
    "for manipulation in manipulated_dirs:\n",
    "    manipulated_videos_path = os.path.join(manipulated_path, manipulation, \"c23/videos\")\n",
    "    for subdir in subdirs:\n",
    "        copy_videos(manipulated_videos_path, os.path.join(dest_dir, subdir, \"manipulated\"), 1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Xvtju_jWUBj"
   },
   "source": [
    "## Extracting frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track_Face():\n",
    "  def __init__(self):\n",
    "    self.model = YOLO('yolov8n.pt', verbose=False)\n",
    "    self.total_frames_extracted = 0\n",
    "\n",
    "  def track(self, frame):\n",
    "    self.img_height, self.img_width, _ = frame.shape\n",
    "    results = self.model(frame)\n",
    "    face_box = self._get_face(results)\n",
    "    cropped_frame = self._crop_face(frame, face_box)\n",
    "    return cropped_frame\n",
    "\n",
    "  def _get_face(self, results):\n",
    "    main_face = None\n",
    "    for res in results[0].boxes:\n",
    "      if res.cls == 0: \n",
    "        face_box = res.xyxy[0].cpu().numpy()\n",
    "        main_face = self._get_main_face(main_face, face_box)\n",
    "    if main_face is None:\n",
    "      return [0, 0, self.img_width, self.img_height]  \n",
    "    return main_face \n",
    "\n",
    "  def _get_main_face(self, main_face, face_box):\n",
    "    if main_face is None:\n",
    "      return face_box\n",
    "    if self._get_area(face_box) > self._get_area(main_face):\n",
    "      return face_box\n",
    "    return main_face\n",
    "\n",
    "  def _get_area(self, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "    return area\n",
    "\n",
    "  def _crop_face(self, frame, face_box):\n",
    "    extended_box = self._extend_box(face_box)\n",
    "    x1, y1, x2, y2 = extended_box\n",
    "    cropped_face = frame[y1:y2, x1:x2]\n",
    "    return cropped_face\n",
    "\n",
    "  def _extend_box(self, box, factor=1.3):\n",
    "    x1, y1, x2, y2 = box\n",
    "    box_width, box_height = x2 - x1, y2 - y1\n",
    "    center_x, center_y = self._calculate_center(box)\n",
    "    extended_box_width, extended_box_height = self._calculate_new_dimensions(box_width, box_height, factor)\n",
    "    return self._calculate_new_coordinates(center_x, center_y, extended_box_width, extended_box_height )\n",
    "\n",
    "  def _calculate_center(self, box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    center_x = x1 + (x2 - x1) / 2\n",
    "    center_y = y1 + (y2 - y1) / 2\n",
    "    return center_x, center_y\n",
    "\n",
    "  def _calculate_new_dimensions(self, width, height, factor):\n",
    "    extended_box_width = width * factor\n",
    "    extended_box_height = height * factor\n",
    "    return extended_box_width, extended_box_height\n",
    "\n",
    "  def _calculate_new_coordinates(self, center_x, center_y, extended_box_width, extended_box_height):\n",
    "    extended_x1 = max(0, int(center_x - extended_box_width / 2))\n",
    "    extended_y1 = max(0, int(center_y - extended_box_height / 2))\n",
    "    extended_x2 = min(self.img_width, int(center_x + extended_box_width / 2))\n",
    "    extended_y2 = min(self.img_height, int(center_y + extended_box_height / 2))\n",
    "    return extended_x1, extended_y1, extended_x2, extended_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOsfR_gDMGBh"
   },
   "outputs": [],
   "source": [
    "class Extract_frames:\n",
    "    def __init__(self, input_directory, output_directory, frames_name, save_frame_interval=4, offset=0, quality=\"c23\"):\n",
    "        self.input_directory = input_directory\n",
    "        self.output_directory = output_directory\n",
    "        self.save_frame_interval = save_frame_interval\n",
    "        self.total_frames_extracted = 0\n",
    "        self.frames_name = frames_name\n",
    "        self.offset = offset\n",
    "        self.quality = quality\n",
    "        self.track_face = Track_Face()\n",
    "\n",
    "    def extract(self):\n",
    "        video_number = 0\n",
    "        self._create_output_folder()\n",
    "        for filename in os.listdir(self.input_directory):\n",
    "            video_path = os.path.join(self.input_directory, filename)\n",
    "            self._capture_frames(video_path, video_number)\n",
    "            video_number += 1\n",
    "        print(f\"Estrazione completata, {self.total_frames_extracted} frames estratti.\")\n",
    "        return\n",
    "\n",
    "    def _create_output_folder(self):\n",
    "        if not os.path.exists(self.output_directory):\n",
    "            os.makedirs(self.output_directory)\n",
    "        return\n",
    "\n",
    "    def _capture_frames(self, video_path, video_number):\n",
    "        capture = cv2.VideoCapture(video_path)\n",
    "        if not capture.isOpened():\n",
    "            print(\"Errore: impossibile aprire il file video\")\n",
    "            return\n",
    "        self._save_frame(capture, video_number)\n",
    "        capture.release()\n",
    "        return\n",
    "\n",
    "    def _save_frame(self, capture, video_number):\n",
    "        frame_number = 0\n",
    "        extracted_frame = 0\n",
    "        while True:\n",
    "            is_a_frame, frame = capture.read()\n",
    "            if not is_a_frame:\n",
    "                break\n",
    "            if (frame_number - self.offset) % self.save_frame_interval == 0:\n",
    "                cropped_frame = self.track_face.track(frame)\n",
    "                resized_frame = cv2.resize(cropped_frame, (299, 299))\n",
    "                frame_filename = os.path.join(self.output_directory, f\"{self.frames_name}_{video_number}_{extracted_frame}_{self.quality}.jpg\")\n",
    "                cv2.imwrite(frame_filename, resized_frame)\n",
    "                extracted_frame += 1\n",
    "            frame_number += 1\n",
    "        print(f\"Estrazione completata per il video numero {video_number}. {extracted_frame} frame estratti e salvati in '{self.output_directory}'.\")\n",
    "        self.total_frames_extracted += extracted_frame\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of frames from original videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/original/c23\", \"dataset/training_set/original\", \"original\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/original/c40\", \"dataset/training_set/original\", \"original\", offset=2, quality=\"c40\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Uo6yeUY_gEEp",
    "outputId": "bde1dd40-53df-43bf-915e-ae1c34b9ba24"
   },
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/original/c23\", \"dataset/validation_set/original\", \"original\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/original/c40\", \"dataset/validation_set/original\", \"original\", offset=2, quality=\"c40\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/original/c23\", \"dataset/test_set/original\", \"original\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/original/c40\", \"dataset/test_set/original\", \"original\", offset=2, quality=\"c40\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of frames from manipulated videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of frames from videos manipulated with deepfakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c23\", \"dataset/training_set/manipulated/deepfake\", \"deepfake\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c40\", \"dataset/training_set/manipulated/deepfake\", \"deepfake\", offset=2, quality=\"c40\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c23\", \"dataset/validation_set/manipulated/deepfake\", \"deepfake\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c40\", \"dataset/validation_set/manipulated/deepfake\", \"deepfake\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c23\", \"dataset/test_set/manipulated/deepfake\", \"deepfake\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c40\", \"dataset/test_set/manipulated/deepfake\", \"deepfake\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of frames from videos manipulated with NeuralTexture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c23\", \"dataset/training_set/manipulated/neuraltexture\", \"neuraltexture\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c40\", \"dataset/training_set/manipulated/neuraltexture\", \"neuraltexture\", offset=2, quality=\"c40\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c23\", \"dataset/validation_set/manipulated/neuraltexture\", \"neuraltexture\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c40\", \"dataset/validation_set/manipulated/neuraltexture\", \"neuraltexture\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c23\", \"dataset/test_set/manipulated/neuraltexture\", \"neuraltexture\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c40\", \"dataset/test_set/manipulated/neuraltexture\", \"neuraltexture\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of frames from videos manipulated with Face2Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c23\", \"dataset/training_set/manipulated/face2face\", \"face2face\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c40\", \"dataset/training_set/manipulated/face2face\", \"face2face\", offset=2, quality=\"c40\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c23\", \"dataset/validation_set/manipulated/face2face\", \"face2face\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c40\", \"dataset/validation_set/manipulated/face2face\", \"face2face\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c23\", \"dataset/test_set/manipulated/face2face\", \"face2face\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c40\", \"dataset/test_set/manipulated/face2face\", \"face2face\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction of frames from videos manipulated with FaceSwap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c23\", \"dataset/training_set/manipulated/faceswap\", \"faceswap\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/training_set/manipulated/c40\", \"dataset/training_set/manipulated/faceswap\", \"faceswap\", offset=2, quality=\"c40\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c23\", \"dataset/validation_set/manipulated/faceswap\", \"faceswap\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/validation_set/manipulated/c40\", \"dataset/validation_set/manipulated/faceswap\", \"faceswap\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c23\", \"dataset/test_set/manipulated/faceswap\", \"faceswap\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames = Extract_frames(\"videos/test_set/manipulated/c40\", \"dataset/test_set/manipulated/faceswap\", \"faceswap\")\n",
    "extract_frames.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk4TalqOz1F4"
   },
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1Z0D7uVhIxL",
    "outputId": "010c3697-f205-47ca-904b-8194f48ba4a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 944585 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    \"dataset/training_set\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=CONFIGURATION[\"SEED\"],\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112837 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = image_dataset_from_directory(\n",
    "    \"dataset/validation_set\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=CONFIGURATION[\"SEED\"],\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 110462 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = image_dataset_from_directory(\n",
    "    \"dataset/test_set\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode = \"binary\",\n",
    "    class_names=CONFIGURATION[\"CLASS_NAMES\"],\n",
    "    image_size=(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "    shuffle=True,\n",
    "    seed=CONFIGURATION[\"SEED\"],\n",
    "    batch_size=CONFIGURATION[\"BATCH_SIZE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn-LjAZPuhcQ"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hvt7rL08vZx5"
   },
   "outputs": [],
   "source": [
    "augmentation_layer  = tf.keras.Sequential([\n",
    "  RandomFlip(\"horizontal\"),\n",
    "  RandomRotation(0.005),\n",
    "  RandomZoom(0.1),\n",
    "  RandomContrast(0.2),\n",
    "  RandomBrightness(0.2)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_WzBPOpuYtS"
   },
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale_layers = tf.keras.Sequential([\n",
    "       Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(image, label):\n",
    "    image = augmentation_layer(image)\n",
    "    image = resize_rescale_layers(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_test(image, label):\n",
    "    image = resize_rescale_layers(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UsUnf8aNubP5"
   },
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    train_dataset\n",
    "    .map(preprocess_train, num_parallel_calls = tf.data.AUTOTUNE)    \n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r7jSsPXDK-bi"
   },
   "outputs": [],
   "source": [
    "validation_dataset = (\n",
    "    validation_dataset.\n",
    "    map(preprocess_validation_test, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    test_dataset\n",
    "    .map(preprocess_validation_test, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,12))\n",
    "\n",
    "for images, labels in validation_dataset.take(1):\n",
    "  for i in range(16):\n",
    "    tensor = labels[i].numpy()\n",
    "    ax = plt.subplot(4,4, i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(CONFIGURATION[\"CLASS_NAMES\"][int(tensor.item())])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert TFDataset in TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    training_dataset.\n",
    "    unbatch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = (\n",
    "    validation_dataset.\n",
    "    unbatch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    test_dataset.\n",
    "    unbatch()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnbatchDataset element_spec=(TensorSpec(shape=(299, 299, 3), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnbatchDataset element_spec=(TensorSpec(shape=(299, 299, 3), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_UnbatchDataset element_spec=(TensorSpec(shape=(299, 299, 3), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    label = tf.cast(label.item(), tf.int64)\n",
    "    \n",
    "    bytes_feature = Feature(\n",
    "        bytes_list=BytesList(value=[image])\n",
    "    )\n",
    "\n",
    "    int_feature = Feature(\n",
    "        int64_list=Int64List(value=[label])\n",
    "    )\n",
    "\n",
    "    example = Example(\n",
    "        features=Features(feature={\n",
    "            'images': bytes_feature,\n",
    "            'labels': int_feature,\n",
    "        })\n",
    "    )\n",
    "\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "    image = tf.io.encode_jpeg(image)\n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord(dataset, num_shards, path):\n",
    "  encoded_dataset = (\n",
    "    dataset.\n",
    "    map(encode_image)\n",
    "  )\n",
    "\n",
    "  for shard_number in range(num_shards):\n",
    "    sharded_dataset = (\n",
    "        encoded_dataset\n",
    "        .shard(num_shards, shard_number)\n",
    "        .as_numpy_iterator()\n",
    "    )\n",
    "    with tf.io.TFRecordWriter(path.format(shard_number)) as file_writer:\n",
    "      for encoded_image, encoded_label in sharded_dataset:\n",
    "        example = create_example(encoded_image, encoded_label)\n",
    "        file_writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_SHARDS = 60\n",
    "NUM_VALIDATION_SHARDS = 15\n",
    "NUM_TEST_SHARDS = 15\n",
    "train_dataset_path = os.path.join(os.getcwd(), 'tfrecords', 'train_dataset', 'shard_{:01d}.tfrecord')\n",
    "validation_dataset_path = os.path.join(os.getcwd(), 'tfrecords', 'validation_dataset', 'shard_{:01d}.tfrecord')\n",
    "test_dataset_path = os.path.join(os.getcwd(), 'tfrecords', 'test_dataset', 'shard_{:01d}.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecord(train_dataset, NUM_TRAIN_SHARDS, train_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecord(validation_dataset, NUM_VALIDATION_SHARDS, validation_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecord(test_dataset, NUM_TEST_SHARDS, test_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconvert TFRecord in TFDataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_train_dataset = tf.data.TFRecordDataset(\n",
    "    filenames =[train_dataset_path.format(p) for p in range(NUM_TRAIN_SHARDS)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_0.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_1.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_2.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_3.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_4.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_5.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_6.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_7.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_8.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_9.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_10.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_11.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_12.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_13.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_14.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_15.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_16.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_17.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_18.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_19.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_20.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_21.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_22.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_23.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_24.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_25.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_26.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_27.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_28.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_29.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_30.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_31.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_32.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_33.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_34.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_35.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_36.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_37.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_38.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_39.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_40.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_41.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_42.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_43.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_44.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_45.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_46.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_47.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_48.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_49.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_50.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_51.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_52.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_53.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_54.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_55.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_56.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_57.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_58.tfrecord', 'c:\\\\Users\\\\nesea\\\\Desktop\\\\DeepFake Detection\\\\tfrecords\\\\train_dataset\\\\shard_59.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "print([train_dataset_path.format(p) for p in range(NUM_TRAIN_SHARDS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_validation_dataset = tf.data.TFRecordDataset(\n",
    "    filenames =[validation_dataset_path.format(p) for p in range(NUM_VALIDATION_SHARDS)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecords(example):\n",
    "\n",
    "    feature_description = {\n",
    "          \"images\": tf.io.FixedLenFeature([], tf.string),\n",
    "          \"labels\": tf.io.FixedLenFeature([], tf.int64),\n",
    "      }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"images\"] = tf.image.convert_image_dtype(\n",
    "        tf.io.decode_jpeg(\n",
    "        example[\"images\"], channels = 3), dtype = tf.float32)\n",
    "\n",
    "    return example[\"images\"], example[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = (\n",
    "    reconstructed_train_dataset\n",
    "    .map(parse_tfrecords)\n",
    "    .batch(CONFIGURATION[\"BATCH_SIZE\"])\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = (\n",
    "    reconstructed_validation_dataset\n",
    "    .map(parse_tfrecords)\n",
    "    .batch(CONFIGURATION[\"BATCH_SIZE\"])\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjLzOuehMdah"
   },
   "source": [
    "## Xception Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9jiCgA0MhqE"
   },
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RE_x1kL9RcRG"
   },
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "  def __init__(self, n_filters, kernel_size, n_strides=1):\n",
    "    super(Convolution, self).__init__(name = \"convolution\")\n",
    "    self.convolution = Conv2D(filters = n_filters, kernel_size = kernel_size, strides = n_strides, padding = \"same\",  activation = \"relu\")\n",
    "    self.batch_normalization = BatchNormalization()\n",
    "\n",
    "  def call(self, x, training = True):\n",
    "    x = self.convolution(x)\n",
    "    x = self.batch_normalization(x, training = training)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4aiK74BTsJ3"
   },
   "source": [
    "### Separable Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xYltSCqoTmPb"
   },
   "outputs": [],
   "source": [
    "class SeparableConvolution(Layer):\n",
    "  def __init__(self, n_filters, kernel_size, n_strides=1):\n",
    "    super(SeparableConvolution, self).__init__(name=\"separable_convolution\")\n",
    "    self.separable_convolution = SeparableConv2D(filters=n_filters, kernel_size=kernel_size, strides=n_strides, padding=\"same\", activation=None)\n",
    "    self.batch_normalization = BatchNormalization()\n",
    "    self.dropout = Dropout(0.2)\n",
    "\n",
    "  def call(self, x, training=True):\n",
    "    x = self.separable_convolution(x)\n",
    "    x = self.batch_normalization(x, training=training)\n",
    "    if training:\n",
    "      x = self.dropout(x, training=training)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsKguFQFhvKS"
   },
   "source": [
    "### Sum Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bfh1riBuhxFs"
   },
   "outputs": [],
   "source": [
    "class SumConvolution(Layer):\n",
    "  def __init__(self, n_filters, kernel_size, n_strides=1):\n",
    "    super(SumConvolution, self).__init__(name = \"sum_convolution\")\n",
    "    self.sum_convolution = Conv2D(filters = n_filters, kernel_size = kernel_size, strides = n_strides, padding = \"same\",  activation = None)\n",
    "    self.batch_normalization = BatchNormalization()\n",
    "\n",
    "  def call(self, x, training = True):\n",
    "    x = self.sum_convolution(x)\n",
    "    x = self.batch_normalization(x, training = training)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYqnbw92eSSA"
   },
   "source": [
    "### Entry Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntryFlow(Layer):\n",
    "    def __init__(self, filters):\n",
    "        super(EntryFlow, self).__init__(name=\"entry_flow\")\n",
    "        self.convolution1 = Convolution(filters[0], 3, 2)\n",
    "        self.convolution2 = Convolution(filters[1], 3) \n",
    "\n",
    "        self.sum_convolution1 = Convolution(filters[2], 1, 2)\n",
    "\n",
    "        self.separable_convolution1 = SeparableConvolution(filters[2], 3)\n",
    "        self.activation = ReLU()\n",
    "        self.separable_convolution2 = SeparableConvolution(filters[2], 3)\n",
    "        self.max_pooling = MaxPool2D(pool_size=(3, 3), strides=2, padding=\"same\")\n",
    "\n",
    "        self.sum_convolution2 = Convolution(filters[3], 1, 2) \n",
    "\n",
    "        self.separable_convolution3 = SeparableConvolution(filters[3], 3)\n",
    "        self.separable_convolution4 = SeparableConvolution(filters[3], 3)\n",
    "                                                                \n",
    "        self.sum_convolution3 = Convolution(filters[4], 1, 2)\n",
    "\n",
    "        self.separable_convolution5 = SeparableConvolution(filters[4], 3)\n",
    "        self.separable_convolution6 = SeparableConvolution(filters[4], 3)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x = self.convolution1(x)        \n",
    "        x = self.convolution2(x)\n",
    "        tensor = self.sum_convolution1(x)\n",
    "\n",
    "        x = self.separable_convolution1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.separable_convolution2(x)\n",
    "        x = self.max_pooling(x)\n",
    "\n",
    "        x = Add()([tensor, x])\n",
    "        tensor = self.sum_convolution2(x)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.separable_convolution3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.separable_convolution4(x)\n",
    "        x = self.max_pooling(x)\n",
    "\n",
    "        x = Add()([tensor, x])\n",
    "        tensor = self.sum_convolution3(x)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.separable_convolution5(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.separable_convolution6(x)\n",
    "        x = self.max_pooling(x)\n",
    "\n",
    "        x = Add()([tensor, x])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpsKZKjolV-H"
   },
   "source": [
    "### Middle Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hy0izkJklT7o"
   },
   "outputs": [],
   "source": [
    "class MiddleFlow(Layer):\n",
    "    def __init__(self, filters):\n",
    "        super(MiddleFlow, self).__init__(name=\"middle_flow\")\n",
    "        self.activation = ReLU()\n",
    "        self.separable_convolution1 = SeparableConvolution(filters[4], 3)\n",
    "        self.separable_convolution2 = SeparableConvolution(filters[4], 3)\n",
    "        self.separable_convolution3 = SeparableConvolution(filters[4], 3)\n",
    "\n",
    "    def call(self, x, tensor, training=True):\n",
    "        for i in range(8):\n",
    "            x = self.activation(x)\n",
    "            x = self.separable_convolution1(x, training = training)\n",
    "            x = self.activation(x)\n",
    "            x = self.separable_convolution2(x, training = training)\n",
    "            x = self.activation(x)\n",
    "            x = self.separable_convolution3(x, training = training)\n",
    "            x = Add()([tensor, x])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTe4tNUtnT_U"
   },
   "source": [
    "### Exit Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6vkpabFunZdj"
   },
   "outputs": [],
   "source": [
    "class ExitFlow(Layer):\n",
    "  def __init__(self, filters):\n",
    "    super(ExitFlow, self).__init__(name = \"exit_flow\")\n",
    "    self.sum_convolution = SumConvolution(filters[5], 1, 2)\n",
    "\n",
    "    self.activation = ReLU()\n",
    "    self.separable_convolution1 = SeparableConvolution(filters[4], 3)\n",
    "    self.separable_convolution2 = SeparableConvolution(filters[5], 3)\n",
    "    self.max_pooling = MaxPool2D(pool_size = (3,3), strides = 2, padding = \"same\")\n",
    "\n",
    "    self.separable_convolution3 = SeparableConvolution(filters[6], 3)\n",
    "    self.separable_convolution4 = SeparableConvolution(filters[7], 3)\n",
    "    self.global_average_pooling = GlobalAveragePooling2D()\n",
    "\n",
    "    #Optionally one may insert fully-connected layers beffore the logistic regression layer, which is explored in the experimental evaluation section (in particular, see figures 7 and 8)\n",
    "\n",
    "  def call(self, x, tensor, training = True):\n",
    "    tensor = self.sum_convolution(tensor)\n",
    "\n",
    "    x = self.activation(x)\n",
    "    x = self.separable_convolution1(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.separable_convolution2(x)\n",
    "    x = self.max_pooling(x)\n",
    "\n",
    "    x = Add()([tensor, x])\n",
    "\n",
    "    x = self.separable_convolution3(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.separable_convolution4(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.global_average_pooling(x)\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP1ox9aeqSdE"
   },
   "source": [
    "### Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "h_VSg36CpLJL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import register_keras_serializable\n",
    "\n",
    "@register_keras_serializable()\n",
    "class XceptionNet(Model):\n",
    "    def __init__(self, filters, trainable=True, name=\"xception_net\", **kwargs):\n",
    "        super(XceptionNet, self).__init__(name=name, **kwargs)\n",
    "        self.entry_flow = EntryFlow(filters)\n",
    "        self.middle_flow = MiddleFlow(filters)\n",
    "        self.exit_flow = ExitFlow(filters)\n",
    "        self.dense = Dense(1, activation=\"sigmoid\")\n",
    "        self.trainable = trainable\n",
    "        self.filters = filters  \n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x = self.entry_flow(x, training=training)\n",
    "        tensor = x\n",
    "        x = self.middle_flow(x, tensor, training=training)\n",
    "        tensor = x\n",
    "        x = self.exit_flow(x, tensor, training=training)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "    def summary(self):\n",
    "        x = Input(shape=(299, 299, 3))\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\n",
    "        return model.summary()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(XceptionNet, self).get_config()\n",
    "        config.update({\n",
    "            \"filters\": self.filters,  \n",
    "            \"trainable\": self.trainable\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        filters = config.pop('filters') \n",
    "        return cls(filters=filters, **config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qaDKteE3qWeQ"
   },
   "outputs": [],
   "source": [
    "model = XceptionNet([32,64,128,256,364,512,1024,2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqDkl0BL2oB7"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qW85MPT6uPYj"
   },
   "source": [
    "### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "VZrcA4_uuQ_Q"
   },
   "outputs": [],
   "source": [
    "adam_optimizer = Adam(\n",
    "    learning_rate = CONFIGURATION[\"LEARNING_RATE\"],\n",
    "    beta_1 = CONFIGURATION[\"BETA_1\"],\n",
    "    beta_2 = CONFIGURATION[\"BETA_2\"],\n",
    "    epsilon = CONFIGURATION[\"EPSILON\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VahJBb5so9_"
   },
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "OgAv_DEZssBT"
   },
   "outputs": [],
   "source": [
    "es_callback = EarlyStopping(\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"auto\",\n",
    "    patience = 3,\n",
    "    restore_best_weights = True,\n",
    "    baseline = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "MX90kFn02pnj"
   },
   "outputs": [],
   "source": [
    "loss_fn = BinaryCrossentropy()\n",
    "optimizer = adam_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "YhL-ydpC3vUj"
   },
   "outputs": [],
   "source": [
    "metrics = [BinaryAccuracy(name = \"accuracy\"),\n",
    "           Recall(name = \"recall\"),\n",
    "           AUC(name = \"auc\")\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = loss_fn, optimizer = optimizer, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 1167889\n",
    "original_samples = 254812\n",
    "manipulated_samples = 913077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {\n",
    "    0: total_samples / original_samples,\n",
    "    1: total_samples /  manipulated_samples\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0MLnUiG4ZOk",
    "outputId": "455f695b-ce99-421b-ac60-70d92846b484"
   },
   "outputs": [],
   "source": [
    "history = model.fit (\n",
    "    training_dataset,\n",
    "    validation_data = validation_dataset,\n",
    "    epochs = 10,\n",
    "    steps_per_epoch = 29190,\n",
    "    validation_steps = 3640,\n",
    "    callbacks = [es_callback, tensorboard_callback],\n",
    "    class_weight = class_weight\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NylJ0UtSZ_rF"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "id": "802PLQiDL4QY"
   },
   "outputs": [],
   "source": [
    "def predict(video_url):\n",
    "  capture = cv2.VideoCapture(video_url)\n",
    "  if not capture.isOpened():\n",
    "    print(\"Errore: impossibile aprire il file video\")\n",
    "    return\n",
    "  tensors = create_frame_tensor(capture)\n",
    "  capture.release()\n",
    "  predictions = []\n",
    "  for tensor in tensors:\n",
    "    frame_prediction = model.predict(tensor)\n",
    "    print(frame_prediction)\n",
    "    if frame_prediction < 0.5:\n",
    "      frame_prediction = \"original\"\n",
    "    else:\n",
    "      frame_prediction = \"manipulated\"\n",
    "    predictions.append(frame_prediction)\n",
    "  result = max(set(predictions), key=predictions.count)\n",
    "  return result\n",
    "\n",
    "def create_frame_tensor(capture):\n",
    "  track_face = TrackFace()\n",
    "  frame_count = 0\n",
    "  frame_tensors = []\n",
    "  while True:\n",
    "    is_a_frame, frame = capture.read()\n",
    "    if not is_a_frame:\n",
    "      break\n",
    "    cropped_frame = track_face.track(frame)\n",
    "    tensor = tf.convert_to_tensor(cropped_frame, dtype=tf.float32)\n",
    "    resize_tensor = tf.keras.Sequential([\n",
    "       Resizing(CONFIGURATION[\"IMAGE_SIZE\"], CONFIGURATION[\"IMAGE_SIZE\"]),\n",
    "       Rescaling(1./255),\n",
    "    ])\n",
    "    tensor = tf.expand_dims(tensor, axis=0)\n",
    "    frame_tensors.append(resize_tensor(tensor))\n",
    "  return frame_tensors\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
